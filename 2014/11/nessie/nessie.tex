\documentclass{amsart}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{fixltx2e}
\usepackage{graphicx}
\usepackage{braket}
\usepackage{literate}
\usepackage[numbers,sort&compress,longnamesfirst]{natbib}
\usepackage[margin=2cm]{geometry}
\providecommand{\alert}[1]{\textbf{#1}}

\title{The Loch Ness Monster}
%\author{William Waites}
\date{\today}

\begin{document}

\maketitle

This simulation of the Master Equation is based on the observation
that the creation and annihilation operators, $a^{\dagger}$ and $a$
are really matrices. It might help with understanding what is going on
in Behr's Molecular Time Machine \citetext{priv.\ comm.}, though
unlike that approach, here we sidestep the combinatoric complications
and simply calculate the result directly.

The direct technique only works without modification for a single
species, but supports any number of reactions. It gets more expensive
with large numbers of particles because computing the matrix
exponential is $\mathcal{O}(n^3)$ in the size of the
matrix~\citep{moler_nineteen_2003} and we need an ever larger one for
more particles.

\input{Master.lhs}
\input{MasterEx.lhs}
\input{Dissipation2.lhs}

\section*{Notes on the method}
\subsection*{Size of the matrices}
The matrices must be bigger than the largest number of particles
expected. Just how much bigger is an interesting question. Here, 10\%
bigger was arbitrarily chosen, but a more rigorous error bound should
be easily obtainable. This bound should hold regardless of the
approximation method.

\subsection*{Approximating exponentials}
When working with floating point numbers, using a Taylor expansion to
approximate a matrix exponential is known to be a bad idea. It is
inefficient and numerically unstable. Using arbitrary precision
numbers it is stable, but still inefficient. The method used here is
as implemented in the \emph{expm} function with Pad\'{e} approximants
and scaling and squaring, which is the ``standard'' method. It is
still expensive, though, and where the task is to multiply a matrix
exponential by a vector there are better ones -- Krylov iteration and
Leja point iteration.

\subsection*{Scaling to multiple reactions}
The method implemented here, though it is not very elegant, scales
immediately to any number of reactions -- the only added cost is in
the initial computation of the Hamiltonian which consists only of
adding and subtracting matrices and in any case is only done once.

\subsection*{Scaling to multiple species}
This is more difficult because it would involve three dimensional
matrices, with an extra axis for species. Some work on the
data-structures involved would be necessary, as they would in any case
for more efficient calculation of the exponentials
themselves.

\subsection*{Parallelism}
There are numerous places here where parallelism can be exploited. The
methods for calculating the exponential could be made to run on
multiple cores and to exploit GPU hardware. Because the solution is
exact, time-steps can also be calculated independently on different
computers in a cluster and the results collated.

\subsection*{Frequency Domain}
There is, in this case mostly due to the initial conditions, the
strong appearance of periodic behaviour. This suggests it might be
profitable to make a fourier transform of the probability
distribution. How does such a transform affect the operators? Can it
be used to make a diagonal Hamiltonian in the frequency basis? If so,
computing the exponential is much simpler.

\bibliographystyle{unsrt}
\bibliography{literature}

\end{document}

